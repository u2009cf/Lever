\section{Background and Motivation}

  In this section, we first introduce the background of batched stream processing and stragglers. Then we deep into the problems existing in traditional straggler mitigation strategies when processing short stream jobs. We also briefly analyze the characteristics of recurring batched stream jobs. Finally, we outline the challenges in pre-scheduling straggler mitigation.

\subsection{Background}

  Batched stream processing system treats a streaming computation as a series of deterministic batch computations on small time intervals \cite{Zaharia2013}. As shown in Figure 2, batched stream processing system (1) receives input data and divides continuous data stream into a series of small batches according to batch interval (a time granularity which is set according to application's response time), (2) when each batch interval arrives, batch jobs are generated for each batch, (3) these jobs would be repeatedly submitted to batch processing engine such as MapReduce or Spark and are re-run periodically to execute with distributed fault tolerance, data locality scheduling and load balancing provided by batch framework automatically. Latency sensitive, micro jobs/tiny tasks and recurring jobs are three dominant characteristics of these systems.
  \begin{figure}[htbp]
    \centering
    \includegraphics[width=0.34\textwidth]{FigureBatchStream}
    \caption{Principles of batched stream processing system}\label{Fig. 2:}
  \end{figure}

  Stragglers have been a primary performance issue for batched stream processing system. Because the whole job must wait for the slowest task. These stragglers cause unproductive wait time for all the others, delaying job completion. Stragglers especially impact short batched stream jobs. On the one hand, short batched stream jobs are more latency-sensitive and require short response time. On the other hand, short batched stream jobs consist of just a few tasks. Such jobs typically get to run all their tasks at less waves. So, if one task straggles, the whole job is significantly delayed. Beyond these, if a job is so severely affected by stragglers that its completion time exceeds one batch interval, it will delay the next job's submission and execution. Stragglers can occur for many reasons, including hardware heterogeneity \cite{Reiss2012}, data skew \cite{Kwon2012}, hardware failures \cite{Ananthanarayanan2010}, energy efficiency \cite{Cheng2015}, resource contention and various OS effects.

\subsection{Problem Analysis}

  Existing methods to straggler mitigation are post-scheduling methods. They fall short in multiple ways. In the following, we analyze four representative approaches. They are Speculative execution, SkewTune, Dolly and Wrangler on behalf of reactive approach with replication, reactive approach without replication, proactive approach with replication and proactive approach without replication respectively.
  \begin{figure*}[htbp]
    \centering
    \subfloat[Speculative Execution]{%
      \includegraphics[width=0.42\textwidth]{Figure5a}
    }
    \subfloat[SkewTune]{%
      \includegraphics[width=0.42\textwidth]{Figure5b}
    }\hfill
    \subfloat[Dolly]{%
      \includegraphics[width=0.42\textwidth]{Figure5c}
    }
    \subfloat[Wrangler]{%
      \includegraphics[width=0.42\textwidth]{Figure5d}
    }\hfill
    \caption{Straggler mitigation under different strategies}
    \label{Fig. 3:}
  \end{figure*}

  Reactive approaches rely on a wait-speculate-re-execute mechanism. Speculative execution \cite{Dean2004} is a widely used reactive approach with replication for straggler mitigation. Speculative execution marks slow running tasks as stragglers and reacts by re-launching multiple copies of them. As soon as one of these copies finishes execution, the rest are killed. Taking Fig. 3(a) as an example, at $t_b$, T1 is marked as stragglers. Then speculative execution launches a copy of T1 on node4. It migrates data block of Task T1 to node4 and starts executing at $t_e$. As we can see, this approach incurs long reactive time and free time for other nodes. It becomes inefficient for short batched stream jobs in two aspects. On one hand, a task must run for a significant amount of time before it can be identified as a straggler. By this time, other tasks of that job have made considerable progress already. On the other hand, the reaction procedure is time-consuming in the context of short stream job with only several seconds of response time, which is at the same magnitude of the time for speculation and migration.

  SkewTune \cite{Kwon2012} is  a reactive approach  without replication. Once one node has an available slot, SkewTune begins to detect data skew and identify stragglers according to each task's remaining time. As shown in Fig. 3(b), when task $T_4$ is completed at $t_b$, SkewTune's detection is triggered and $T_1$ is identified as straggler. Then, $T_1$ is selected for skew mitigation due to its long remaining progress. After that, SkewTune scans $T_1$'s remaining input data and repartitions its remaining workload into $T_{1b}$, $T_{1c}$ and $T_{1d}$, which  are  migrated to nodes $node_4$, $node_3$ and $node_2$, respectively. From $t_b$ to $t_e$, this procedure incurs approximately 2 seconds overhead for a task with 2MB's input data. This overhead grows linearly with the size of the task's input data, and is intolerant for short stream jobs which must be completed in several seconds or sub-seconds. Furthermore, before SkewTune begins, task $T_1$ has already slowed down. Therefore, basically SkewTune is also in wait-and-speculate paradigm.

  Dolly \cite{Ananthanarayanan2013} is a proactive strategy by full cloning of small jobs. Dolly launches multiple clones of every task of a job and only use the result of the clone that finishes first. As shown in Fig. 3(c), Dolly launches two clones of Task $T_1$ and $T_2$ in $node_1, node_3$ and $node_2, node_4$ respectively. After $T_1$ and $T_2$ complete, Dolly schedules $T_3$ and also launches two clones in $node_3$ and $node_4$. Although multiple clones for each task are spawned, only one of them is effective, incurring significant waste of resources.

  Unlike Dolly, Wrangler \cite{Yadwadkar2014} is a proactive strategy without replication. It predicts stragglers using an interpretable linear modeling technique based on cluster resource usage counters and uses these predictions to inform scheduling decisions. For example, in Fig. 3(d), Wrangler analyzes statistics and make predictions about stragglers at $t_b$. Node1 and node2 are identified as stragglers. Then Wrangler schedules and migrates T5 in node1, T6 in node2 onto node3 and node4 respectively. Although this approach can avoid successor possible stragglers, it remains taking hysteretic actions for short batched stream jobs. Because short batched stream jobs always consist of a few tasks and get to run at once. Furthermore, Wrangler can't act until some nodes show some indications of stragglers(eg. high cpu utilization, heavy memory access, network congestion). It also need to migrate data block at runtime regardless of data locality.

  In summary, existing post-scheduling approaches fall short when processing short stream jobs. Reactive approaches act after stragglers have occurred. At that time, tasks have already slowed down. Replication-based proactive approaches incur extra resource and increase 2x load. Proactive approaches without replication also need to migrate data block at runtime. Consequently, it is desirable to design a straggler mitigation strategy that can react as early as possible, without incurring too much extra resource overhead.

\subsection{Recurring Batched Stream Jobs}

  Batched stream jobs are periodic in nature, hence are typically recurring with stable code and data properties. Many characteristics, such as application logic, stragglers, resource utilization, scheduling and other operations, are statistically similar to the previous execution when running on newer data of the same stream \cite{Agarwal} \cite{Grandl2016} \cite{Jyothi2016}. The history of prior runs for recurring jobs can be used to estimate tasks' durations and predict stragglers.

  Considering the following two points that 1) Straggler characteristics can often be accurately predicted in recurring jobs and 2) Job input data can be assigned in designated locations before scheduling to avoid straggler in advance, we can try to mitigate stragglers via pre-scheduling by exploring the statistical similarity in short batched stream jobs.

\subsection{Challenges}

  We are faced with three challenges:

  Challenge 1 How to identify potential stragglers?

  Pre-scheduling need to know which nodes will be stragglers in next batch. Unlike traditional post-scheduling ways which identify stragglers based on runtime-aware decisions, pre-scheduling takes actions in batching model without task scheduling information. All that pre-scheduling can use is the historical information of recurring jobs. But only analyzing historical information is not enough due to the variability of stream load. Accurately identifying potential stragglers is a prerequisite step for eliminating stragglers.

  Challenge 2 How to determine node's capability?

  Traditional post-scheduling methods have resource utilization view at runtime and can schedule straggler tasks to that node which has free slots. However, in pre-scheduling solutions, it is impossible because we make pre-scheduling decisions before task scheduling. We should define a metric to determine how much data we should pre-schedule to target node. This metric represents to which degree this node will has free slots in future scheduling. So we introduce the notion of node's capability.

  Challenges 3 How to select suitable helpers for data assignment?

  To pre-schedule input data as evenly as possible to eliminate stragglers, pre-scheduling need to choose some helpers which are eligible to provide assistance. These helpers should be underloaded and have spare capability in next batch to deal with extra work assigned to them. As explained in Challenge 2, pre-scheduling don't have the perspective of runtime resource utilization. It brings great challenges when we make decisions about which nodes can afford extra load. How to choose suitable helpers with being unaware of task scheduling is a challenging problem.

  \begin{figure}[htbp]
    \centering
    \includegraphics[width=0.35\textwidth]{FigureArchitecture}
    \caption{Architecture of Lever}
    \label{Fig. 4:}
  \end{figure}
  To overcome these challenges, we design and implement a prototype of Lever.